{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on scripts/data_nyt.py \n",
    "\n",
    "This will create the data set containing the full corpus, so the topics of the full corpus can be determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', 'came', 'can', 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', 'different', 'do', 'does', 'doing', 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', 'happens', 'hardly', 'has', 'have', 'having', 'he', 'hello', 'help', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', 'way', 'we', 'welcome', 'well', 'went', 'were', 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', 'wonder', 'would', 'would', 'x', 'y', 'yes', 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero', '', 'inspiration', 'challenges', 'accomplishments', 'hackathon', 'eu']\n"
     ]
    }
   ],
   "source": [
    "# Maximum / minimum document frequency\n",
    "max_df = 0.75\n",
    "min_df = 3  # choose desired value for min_df\n",
    "\n",
    "# Train/Test Proportions\n",
    "TrProp = 0.95\n",
    "TsProp = 1.00\n",
    "\n",
    "# Read stopwords\n",
    "with open('scripts/stops.txt', 'r') as f:\n",
    "    stops = f.read().split('\\n')\n",
    "\n",
    "\n",
    "stops.extend(['inspiration', 'challenges', 'accomplishments', 'hackathon', 'eu'])\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading text file...\n",
      "  Challenge SubChallenge                                            ProjURL  \\\n",
      "0    Health    Equipment                  https://devpost.com/software/evam   \n",
      "1    Health    Equipment            https://devpost.com/software/nanomaskcz   \n",
      "2    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "3    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "4    Health    Equipment  https://devpost.com/software/innovative-respir...   \n",
      "\n",
      "                     title                                               text  \\\n",
      "0                    EVAM   Inspiration\\nThere is a huge shortage in the s...   \n",
      "1               NanomaskCZ  Inspiration\\nThe story of Technical University...   \n",
      "2  Ecological medical coat  Inspiration\\nWhat it does\\nEconomic medical co...   \n",
      "3  Ecological medical coat  Inspiration\\nThe simplicity and the economical...   \n",
      "4           Respire Action  Inspiration\\n• A recent study shows that over ...   \n",
      "\n",
      "   oldIndex  \n",
      "0         0  \n",
      "1         1  \n",
      "2         2  \n",
      "3         3  \n",
      "4         4  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2159 entries, 0 to 2158\n",
      "Data columns (total 6 columns):\n",
      "Challenge       2159 non-null object\n",
      "SubChallenge    2159 non-null object\n",
      "ProjURL         2159 non-null object\n",
      "title           2159 non-null object\n",
      "text            2069 non-null object\n",
      "oldIndex        2159 non-null int64\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 101.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "print('reading text file...')\n",
    "\n",
    "# The file with all the project data is in \"data/all_data.tsv\"\n",
    "tsvFile = \"data/all_data.tsv\"\n",
    "df = pd.read_csv(tsvFile, sep=\"\\t\")\n",
    "\n",
    "df = df.assign(oldIndex = df.index)\n",
    "\n",
    "print(df.head())\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2069 entries, 0 to 2068\n",
      "Data columns (total 6 columns):\n",
      "Challenge       2069 non-null object\n",
      "SubChallenge    2069 non-null object\n",
      "ProjURL         2069 non-null object\n",
      "title           2069 non-null object\n",
      "text            2069 non-null object\n",
      "oldIndex        2069 non-null int64\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 97.1+ KB\n",
      "None\n",
      "  Challenge SubChallenge                                            ProjURL  \\\n",
      "0    Health    Equipment                  https://devpost.com/software/evam   \n",
      "1    Health    Equipment            https://devpost.com/software/nanomaskcz   \n",
      "2    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "3    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "4    Health    Equipment  https://devpost.com/software/innovative-respir...   \n",
      "\n",
      "                     title                                               text  \\\n",
      "0                    EVAM   Inspiration\\nThere is a huge shortage in the s...   \n",
      "1               NanomaskCZ  Inspiration\\nThe story of Technical University...   \n",
      "2  Ecological medical coat  Inspiration\\nWhat it does\\nEconomic medical co...   \n",
      "3  Ecological medical coat  Inspiration\\nThe simplicity and the economical...   \n",
      "4           Respire Action  Inspiration\\n• A recent study shows that over ...   \n",
      "\n",
      "   oldIndex  \n",
      "0         0  \n",
      "1         1  \n",
      "2         2  \n",
      "3         3  \n",
      "4         4  \n"
     ]
    }
   ],
   "source": [
    "# drop rows with NA text, keep track of old Index Numbers\n",
    "newDF = df[df['text'].notnull()].reset_index(drop = True)\n",
    "\n",
    "print(newDF.info())\n",
    "print(newDF.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows have been dropped?\n",
    "len(set(df.oldIndex).difference(set(newDF.oldIndex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = newDF.text.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the corpus into sentences before saving the corpus:\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "\n",
    "sent1 = [sent_tokenize(doc) for doc in docs]\n",
    "\n",
    "# flatten the list:\n",
    "sent1 = list(chain.from_iterable(sent1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many people forgot to punctuate between sentences, and used EOL instead.\n",
    "# So lets also split sentences by EOL\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "def clean_sentence(s):\n",
    "    newS = ' '.join(word.strip(string.punctuation) for word in s.split())  # get rid of extra punctuation\n",
    "    yield newS\n",
    "    \n",
    "sentences = [clean_sentence(s) for l in sent1 for s in re.split(\"\\n\", l)]\n",
    "\n",
    "# Again, flatten the list\n",
    "sentences = list(chain.from_iterable(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the sentences to a .txt file\n",
    "with open(\"data/fullcorpus.txt\", 'w') as f:\n",
    "    for v in sentences:\n",
    "        f.write(v + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counting document frequency of words...\n"
     ]
    }
   ],
   "source": [
    "# Create count vectorizer\n",
    "print('counting document frequency of words...')\n",
    "cvectorizer = CountVectorizer(min_df=min_df, max_df=max_df, stop_words=None)\n",
    "cvz = cvectorizer.fit_transform(docs).sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building the vocabulary...\n",
      "  initial vocabulary size: 12247\n"
     ]
    }
   ],
   "source": [
    "# Get vocabulary\n",
    "print('building the vocabulary...')\n",
    "sum_counts = cvz.sum(axis=0)\n",
    "v_size = sum_counts.shape[1]\n",
    "sum_counts_np = np.zeros(v_size, dtype=int)\n",
    "for v in range(v_size):\n",
    "    sum_counts_np[v] = sum_counts[0,v]\n",
    "word2id = dict([(w, cvectorizer.vocabulary_.get(w)) for w in cvectorizer.vocabulary_])\n",
    "id2word = dict([(cvectorizer.vocabulary_.get(w), w) for w in cvectorizer.vocabulary_])\n",
    "del cvectorizer\n",
    "print('  initial vocabulary size: {}'.format(v_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary size after removing stopwords from list: 11805\n"
     ]
    }
   ],
   "source": [
    "# Sort elements in vocabulary\n",
    "idx_sort = np.argsort(sum_counts_np)\n",
    "vocab_aux = [id2word[idx_sort[cc]] for cc in range(v_size)]\n",
    "\n",
    "# Filter out stopwords (if any)\n",
    "vocab_aux = [w for w in vocab_aux if w not in stops]\n",
    "print('  vocabulary size after removing stopwords from list: {}'.format(len(vocab_aux)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary and inverse dictionary\n",
    "vocab = vocab_aux\n",
    "# del vocab_aux\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Challenge SubChallenge                                            ProjURL  \\\n",
      "0    Health    Equipment                  https://devpost.com/software/evam   \n",
      "1    Health    Equipment            https://devpost.com/software/nanomaskcz   \n",
      "2    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "3    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "4    Health    Equipment  https://devpost.com/software/innovative-respir...   \n",
      "\n",
      "                     title                                               text  \\\n",
      "0                    EVAM   Inspiration\\nThere is a huge shortage in the s...   \n",
      "1               NanomaskCZ  Inspiration\\nThe story of Technical University...   \n",
      "2  Ecological medical coat  Inspiration\\nWhat it does\\nEconomic medical co...   \n",
      "3  Ecological medical coat  Inspiration\\nThe simplicity and the economical...   \n",
      "4           Respire Action  Inspiration\\n• A recent study shows that over ...   \n",
      "\n",
      "   oldIndex                                             fullDS  \n",
      "0         0  [11434, 10677, 11466, 11152, 11753, 10756, 393...  \n",
      "1         1  [10628, 11725, 9162, 11460, 11640, 11651, 3967...  \n",
      "2         2  [11746, 10822, 11491, 9551, 11438, 11756, 9138...  \n",
      "3         3  [9389, 9740, 10793, 11621, 10928, 5426, 9138, ...  \n",
      "4         4  [11037, 11349, 11093, 11747, 11561, 11669, 114...  \n"
     ]
    }
   ],
   "source": [
    "# add a column to newDF, with the text converted into word IDs\n",
    "docs_full = [[word2id[w] for w in doc.split() if w in word2id] for doc in docs]\n",
    "newDF = newDF.assign(fullDS = docs_full)\n",
    "print(newDF.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  current number of rows: 1961 [this should be equal to 1961]\n"
     ]
    }
   ],
   "source": [
    "# get rid of rows (documents) that contain too few words\n",
    "def not_empty(in_docs, minDocLen = 3):\n",
    "    num_docs = len(in_docs)\n",
    "    return[j for j in range(num_docs) if len(in_docs[j]) > minDocLen]\n",
    "\n",
    "keepIdx = not_empty(newDF.fullDS)\n",
    "removeIdx = list(set(newDF.index).difference(keepIdx))\n",
    "\n",
    "prevLen = newDF.shape[0]\n",
    "tempLen = len(set(newDF.index).difference(keepIdx))\n",
    "\n",
    "newDF = newDF.loc[keepIdx].reset_index(drop = True)\n",
    "\n",
    "print('  current number of rows: {} [this should be equal to {}]'.format(newDF.shape[0], prevLen - tempLen))\n",
    "\n",
    "\n",
    "del prevLen\n",
    "del tempLen\n",
    "del keepIdx\n",
    "del removeIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing documents and splitting into train/test/valid...\n",
      "1961\n",
      "  training set size: 1862\n",
      "  validation set size: 99\n",
      "  test set size: 1961\n",
      "  total documents size: 1961 (this should be equal to 1961)\n"
     ]
    }
   ],
   "source": [
    "# Split in train/test/valid\n",
    "print('tokenizing documents and splitting into train/test/valid...')\n",
    "num_docs = newDF.shape[0]\n",
    "print(num_docs)\n",
    "trSize = int(np.floor(TrProp*num_docs))\n",
    "tsSize = int(np.floor(TsProp*num_docs))\n",
    "vaSize = int(num_docs - trSize)\n",
    "del cvz\n",
    "idx_permute = np.random.permutation(num_docs).astype(int)\n",
    "\n",
    "print('  training set size: {}'.format(trSize))\n",
    "print('  validation set size: {}'.format(vaSize))\n",
    "print('  test set size: {}'.format(tsSize))\n",
    "print('  total documents size: {} (this should be equal to {})'.format(num_docs, trSize + vaSize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1961 entries, 0 to 1960\n",
      "Data columns (total 8 columns):\n",
      "Challenge       1961 non-null object\n",
      "SubChallenge    1961 non-null object\n",
      "ProjURL         1961 non-null object\n",
      "title           1961 non-null object\n",
      "text            1961 non-null object\n",
      "oldIndex        1961 non-null int64\n",
      "fullDS          1961 non-null object\n",
      "newIndex        1961 non-null int64\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 122.6+ KB\n"
     ]
    }
   ],
   "source": [
    "newDF = newDF.assign(newIndex = newDF.index)\n",
    "newDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1961\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Challenge</th>\n",
       "      <th>SubChallenge</th>\n",
       "      <th>ProjURL</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>oldIndex</th>\n",
       "      <th>fullDS</th>\n",
       "      <th>newIndex</th>\n",
       "      <th>permuteIdx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Health</td>\n",
       "      <td>Equipment</td>\n",
       "      <td>https://devpost.com/software/evam</td>\n",
       "      <td>EVAM</td>\n",
       "      <td>Inspiration\\nThere is a huge shortage in the s...</td>\n",
       "      <td>0</td>\n",
       "      <td>[11434, 10677, 11466, 11152, 11753, 10756, 393...</td>\n",
       "      <td>0</td>\n",
       "      <td>1003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Health</td>\n",
       "      <td>Equipment</td>\n",
       "      <td>https://devpost.com/software/nanomaskcz</td>\n",
       "      <td>NanomaskCZ</td>\n",
       "      <td>Inspiration\\nThe story of Technical University...</td>\n",
       "      <td>1</td>\n",
       "      <td>[10628, 11725, 9162, 11460, 11640, 11651, 3967...</td>\n",
       "      <td>1</td>\n",
       "      <td>1327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Health</td>\n",
       "      <td>Equipment</td>\n",
       "      <td>https://devpost.com/software/ecological-medica...</td>\n",
       "      <td>Ecological medical coat</td>\n",
       "      <td>Inspiration\\nWhat it does\\nEconomic medical co...</td>\n",
       "      <td>2</td>\n",
       "      <td>[11746, 10822, 11491, 9551, 11438, 11756, 9138...</td>\n",
       "      <td>2</td>\n",
       "      <td>1434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Health</td>\n",
       "      <td>Equipment</td>\n",
       "      <td>https://devpost.com/software/ecological-medica...</td>\n",
       "      <td>Ecological medical coat</td>\n",
       "      <td>Inspiration\\nThe simplicity and the economical...</td>\n",
       "      <td>3</td>\n",
       "      <td>[9389, 9740, 10793, 11621, 10928, 5426, 9138, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Health</td>\n",
       "      <td>Equipment</td>\n",
       "      <td>https://devpost.com/software/innovative-respir...</td>\n",
       "      <td>Respire Action</td>\n",
       "      <td>Inspiration\\n• A recent study shows that over ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[11037, 11349, 11093, 11747, 11561, 11669, 114...</td>\n",
       "      <td>4</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Challenge SubChallenge                                            ProjURL  \\\n",
       "0    Health    Equipment                  https://devpost.com/software/evam   \n",
       "1    Health    Equipment            https://devpost.com/software/nanomaskcz   \n",
       "2    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
       "3    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
       "4    Health    Equipment  https://devpost.com/software/innovative-respir...   \n",
       "\n",
       "                     title                                               text  \\\n",
       "0                    EVAM   Inspiration\\nThere is a huge shortage in the s...   \n",
       "1               NanomaskCZ  Inspiration\\nThe story of Technical University...   \n",
       "2  Ecological medical coat  Inspiration\\nWhat it does\\nEconomic medical co...   \n",
       "3  Ecological medical coat  Inspiration\\nThe simplicity and the economical...   \n",
       "4           Respire Action  Inspiration\\n• A recent study shows that over ...   \n",
       "\n",
       "   oldIndex                                             fullDS  newIndex  \\\n",
       "0         0  [11434, 10677, 11466, 11152, 11753, 10756, 393...         0   \n",
       "1         1  [10628, 11725, 9162, 11460, 11640, 11651, 3967...         1   \n",
       "2         2  [11746, 10822, 11491, 9551, 11438, 11756, 9138...         2   \n",
       "3         3  [9389, 9740, 10793, 11621, 10928, 5426, 9138, ...         3   \n",
       "4         4  [11037, 11349, 11093, 11747, 11561, 11669, 114...         4   \n",
       "\n",
       "   permuteIdx  \n",
       "0        1003  \n",
       "1        1327  \n",
       "2        1434  \n",
       "3          60  \n",
       "4         706  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#newDF = newDF.assign(permuteIdx = newDF.apply(lambda x: np.where(x.newIndex == idx_permute)[0][0], axis = 1))\n",
    "print(len(idx_permute))\n",
    "def get_permuteIndex(n):\n",
    "    N = np.where(n == idx_permute)\n",
    "    return N[0][0]\n",
    "    \n",
    "newDF = newDF.assign(permuteIdx = newDF.apply(lambda x: get_permuteIndex(x.newIndex), axis = 1))\n",
    "newDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vocabulary after removing words not in train: 10684\n"
     ]
    }
   ],
   "source": [
    "# Remove words not in train_data\n",
    "vocab = list(set([w for idx_d in range(trSize) for w in newDF.text[idx_permute[idx_d]].split() if w in word2id]))\n",
    "word2id = dict([(w, j) for j, w in enumerate(vocab)])\n",
    "id2word = dict([(j, w) for j, w in enumerate(vocab)])\n",
    "print('  vocabulary after removing words not in train: {}'.format(len(vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vocab = [[i for i in doc if i in id2word] for doc in newDF.fullDS]\n",
    "newDF = newDF.assign(vocabDS = docs_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "keepIdx = not_empty(newDF.vocabDS, minDocLen = 1)\n",
    "removeIdx = list(set(range(len(docs_vocab))).difference(keepIdx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(keepIdx))\n",
    "len(removeIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_tr = [idx_permute[idx_d] for idx_d in range(trSize) if idx_permute[idx_d] in keepIdx]\n",
    "idx_va = [idx_permute[idx_d] for idx_d in range(trSize, trSize + vaSize) if idx_permute[idx_d] in keepIdx]\n",
    "idx_ts = [idx_permute[idx_d] for idx_d in range(num_docs) if idx_permute[idx_d] in keepIdx]\n",
    "\n",
    "docs_tr = [newDF.vocabDS[idx_d] for idx_d in idx_tr]\n",
    "docs_ts = [newDF.vocabDS[idx_d] for idx_d in idx_ts]\n",
    "docs_va = [newDF.vocabDS[idx_d] for idx_d in idx_va]\n",
    "\n",
    "print('  number of documents (train): {} [this should be less than {} and equal {}]'.format(len(docs_tr), trSize, len(idx_tr)))\n",
    "print('  number of documents (test): {} [this should be less than {} and equal {}]'.format(len(docs_ts), tsSize, len(idx_ts)))\n",
    "print('  number of documents (valid): {} [this should be less than {} and equal {}]'.format(len(docs_va), vaSize, len(idx_va)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Challenge SubChallenge                                            ProjURL  \\\n",
      "0    Health    Equipment                  https://devpost.com/software/evam   \n",
      "1    Health    Equipment            https://devpost.com/software/nanomaskcz   \n",
      "2    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "3    Health    Equipment  https://devpost.com/software/ecological-medica...   \n",
      "4    Health    Equipment  https://devpost.com/software/innovative-respir...   \n",
      "\n",
      "                     title                                               text  \\\n",
      "0                    EVAM   Inspiration\\nThere is a huge shortage in the s...   \n",
      "1               NanomaskCZ  Inspiration\\nThe story of Technical University...   \n",
      "2  Ecological medical coat  Inspiration\\nWhat it does\\nEconomic medical co...   \n",
      "3  Ecological medical coat  Inspiration\\nThe simplicity and the economical...   \n",
      "4           Respire Action  Inspiration\\n• A recent study shows that over ...   \n",
      "\n",
      "   oldIndex                                             fullDS  newIndex  \\\n",
      "0         0  [11434, 10677, 11466, 11152, 11753, 10756, 393...         0   \n",
      "1         1  [10628, 11725, 9162, 11460, 11640, 11651, 3967...         1   \n",
      "2         2  [11746, 10822, 11491, 9551, 11438, 11756, 9138...         2   \n",
      "3         3  [9389, 9740, 10793, 11621, 10928, 5426, 9138, ...         3   \n",
      "4         4  [11037, 11349, 11093, 11747, 11561, 11669, 114...         4   \n",
      "\n",
      "   permuteIdx                                            vocabDS dsType  \n",
      "0        1003  [10677, 3936, 8036, 9241, 10467, 9241, 9241, 6...     Tr  \n",
      "1        1327  [10628, 9162, 3967, 10266, 8010, 8349, 8515, 6...     Tr  \n",
      "2        1434  [9551, 9138, 8515, 3114, 508, 5037, 508, 508, ...     Tr  \n",
      "3          60  [9389, 9740, 5426, 9138, 9551, 8515, 9287, 105...     Tr  \n",
      "4         706  [9270, 10004, 8291, 9244, 987, 9250, 8369, 105...     Tr  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1961 entries, 0 to 1960\n",
      "Data columns (total 11 columns):\n",
      "Challenge       1961 non-null object\n",
      "SubChallenge    1961 non-null object\n",
      "ProjURL         1961 non-null object\n",
      "title           1961 non-null object\n",
      "text            1961 non-null object\n",
      "oldIndex        1961 non-null int64\n",
      "fullDS          1961 non-null object\n",
      "newIndex        1961 non-null int64\n",
      "permuteIdx      1961 non-null int64\n",
      "vocabDS         1961 non-null object\n",
      "dsType          1961 non-null object\n",
      "dtypes: int64(3), object(8)\n",
      "memory usage: 168.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def assign_type(Idx):\n",
    "    pIdx = newDF.permuteIdx[Idx]\n",
    "    if Idx in removeIdx:\n",
    "        dsType = \"NA\"\n",
    "    elif (pIdx < trSize):\n",
    "        dsType = \"Tr\"\n",
    "    else:\n",
    "        dsType = \"Va\"\n",
    "        \n",
    "    return dsType\n",
    "\n",
    "\n",
    "newDF = newDF.assign(dsType = [assign_type(Idx) for Idx in newDF.index])\n",
    "\n",
    "print(newDF.head())\n",
    "\n",
    "print(newDF.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  number of documents (train): 1848 [this should be equal to 1848]\n",
      "  number of documents (test): 1945 [this should be equal to 1945]\n",
      "  number of documents (valid): 97 [this should be equal to 97]\n"
     ]
    }
   ],
   "source": [
    "trSize1 = sum(newDF.dsType == \"Tr\")\n",
    "vaSize1 = sum(newDF.dsType == \"Va\")\n",
    "tsSize1 = sum(newDF.dsType != \"NA\")\n",
    "\n",
    "print('  number of documents (train): {} [this should be equal to {}]'.format(len(docs_tr), trSize1))\n",
    "print('  number of documents (test): {} [this should be equal to {}]'.format(len(docs_ts), tsSize1))\n",
    "print('  number of documents (valid): {} [this should be equal to {}]'.format(len(docs_va), vaSize1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test set in 2 halves\n",
    "print('splitting test documents in 2 halves...')\n",
    "docs_ts_h1 = [[w for i,w in enumerate(doc) if i<=len(doc)/2.0-1] for doc in docs_ts]\n",
    "docs_ts_h2 = [[w for i,w in enumerate(doc) if i>len(doc)/2.0-1] for doc in docs_ts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting lists of words and doc_indices\n",
    "print('creating lists of words...')\n",
    "\n",
    "def create_list_words(in_docs):\n",
    "    return [x for y in in_docs for x in y]\n",
    "\n",
    "words_tr = create_list_words(docs_tr)\n",
    "words_ts = create_list_words(docs_ts)\n",
    "words_ts_h1 = create_list_words(docs_ts_h1)\n",
    "words_ts_h2 = create_list_words(docs_ts_h2)\n",
    "words_va = create_list_words(docs_va)\n",
    "\n",
    "print('  len(words_tr): ', len(words_tr))\n",
    "print('  len(words_ts): ', len(words_ts))\n",
    "print('  len(words_ts_h1): ', len(words_ts_h1))\n",
    "print('  len(words_ts_h2): ', len(words_ts_h2))\n",
    "print('  len(words_va): ', len(words_va))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get doc indices\n",
    "print('getting doc indices...')\n",
    "\n",
    "def create_doc_indices(in_docs):\n",
    "    aux = [[j for i in range(len(doc))] for j, doc in enumerate(in_docs)]\n",
    "    return [int(x) for y in aux for x in y]\n",
    "\n",
    "doc_indices_tr = create_doc_indices(docs_tr)\n",
    "doc_indices_ts = create_doc_indices(docs_ts)\n",
    "doc_indices_ts_h1 = create_doc_indices(docs_ts_h1)\n",
    "doc_indices_ts_h2 = create_doc_indices(docs_ts_h2)\n",
    "doc_indices_va = create_doc_indices(docs_va)\n",
    "\n",
    "print('  len(np.unique(doc_indices_tr)): {} [this should be {}]'.format(len(np.unique(doc_indices_tr)), len(docs_tr)))\n",
    "print('  len(np.unique(doc_indices_ts)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts)), len(docs_ts)))\n",
    "print('  len(np.unique(doc_indices_ts_h1)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h1)), len(docs_ts_h1)))\n",
    "print('  len(np.unique(doc_indices_ts_h2)): {} [this should be {}]'.format(len(np.unique(doc_indices_ts_h2)), len(docs_ts_h2)))\n",
    "print('  len(np.unique(doc_indices_va)): {} [this should be {}]'.format(len(np.unique(doc_indices_va)), len(docs_va)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents in each set\n",
    "n_docs_tr = len(docs_tr)\n",
    "n_docs_ts = len(docs_ts)\n",
    "n_docs_ts_h1 = len(docs_ts_h1)\n",
    "n_docs_ts_h2 = len(docs_ts_h2)\n",
    "n_docs_va = len(docs_va)\n",
    "\n",
    "# Remove unused variables\n",
    "del docs_tr\n",
    "del docs_ts\n",
    "del docs_ts_h1\n",
    "del docs_ts_h2\n",
    "del docs_va\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(doc_indices_tr))\n",
    "print(len(words_tr))\n",
    "print(n_docs_tr)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bow representation\n",
    "print('creating bow representation...')\n",
    "\n",
    "def create_bow(doc_indices, words, n_docs, vocab_size):\n",
    "    return sparse.coo_matrix(([1]*len(doc_indices),(doc_indices, words)), shape=(n_docs, vocab_size)).tocsr()\n",
    "\n",
    "bow_tr = create_bow(doc_indices_tr, words_tr, n_docs_tr, len(vocab))\n",
    "bow_ts = create_bow(doc_indices_ts, words_ts, n_docs_ts, len(vocab))\n",
    "bow_ts_h1 = create_bow(doc_indices_ts_h1, words_ts_h1, n_docs_ts_h1, len(vocab))\n",
    "bow_ts_h2 = create_bow(doc_indices_ts_h2, words_ts_h2, n_docs_ts_h2, len(vocab))\n",
    "bow_va = create_bow(doc_indices_va, words_va, n_docs_va, len(vocab))\n",
    "\n",
    "del words_tr\n",
    "del words_ts\n",
    "del words_ts_h1\n",
    "del words_ts_h2\n",
    "del words_va\n",
    "del doc_indices_tr\n",
    "del doc_indices_ts\n",
    "del doc_indices_ts_h1\n",
    "del doc_indices_ts_h2\n",
    "del doc_indices_va\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vocabulary to file\n",
    "import os\n",
    "\n",
    "path_save = './min_df_' + str(min_df) + '/'\n",
    "if not os.path.isdir(path_save):\n",
    "    os.system('mkdir -p ' + path_save)\n",
    "\n",
    "with open(path_save + 'vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "del vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_save + 'dataDF.pkl', 'wb') as f:\n",
    "    pickle.dump(newDF, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split bow into token/value pairs\n",
    "print('splitting bow into token/value pairs and saving to disk...')\n",
    "\n",
    "def split_bow(bow_in, n_docs):\n",
    "    indices = [[w for w in bow_in[doc,:].indices] for doc in range(n_docs)]\n",
    "    counts = [[c for c in bow_in[doc,:].data] for doc in range(n_docs)]\n",
    "    return indices, counts\n",
    "\n",
    "bow_tr_tokens, bow_tr_counts = split_bow(bow_tr, n_docs_tr)\n",
    "savemat(path_save + 'bow_tr_tokens', {'tokens': bow_tr_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_tr_counts', {'counts': bow_tr_counts}, do_compression=True)\n",
    "del bow_tr\n",
    "del bow_tr_tokens\n",
    "del bow_tr_counts\n",
    "\n",
    "bow_ts_tokens, bow_ts_counts = split_bow(bow_ts, n_docs_ts)\n",
    "savemat(path_save + 'bow_ts_tokens', {'tokens': bow_ts_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_counts', {'counts': bow_ts_counts}, do_compression=True)\n",
    "del bow_ts\n",
    "del bow_ts_tokens\n",
    "del bow_ts_counts\n",
    "\n",
    "bow_ts_h1_tokens, bow_ts_h1_counts = split_bow(bow_ts_h1, n_docs_ts_h1)\n",
    "savemat(path_save + 'bow_ts_h1_tokens', {'tokens': bow_ts_h1_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h1_counts', {'counts': bow_ts_h1_counts}, do_compression=True)\n",
    "del bow_ts_h1\n",
    "del bow_ts_h1_tokens\n",
    "del bow_ts_h1_counts\n",
    "\n",
    "bow_ts_h2_tokens, bow_ts_h2_counts = split_bow(bow_ts_h2, n_docs_ts_h2)\n",
    "savemat(path_save + 'bow_ts_h2_tokens', {'tokens': bow_ts_h2_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_ts_h2_counts', {'counts': bow_ts_h2_counts}, do_compression=True)\n",
    "del bow_ts_h2\n",
    "del bow_ts_h2_tokens\n",
    "del bow_ts_h2_counts\n",
    "\n",
    "bow_va_tokens, bow_va_counts = split_bow(bow_va, n_docs_va)\n",
    "savemat(path_save + 'bow_va_tokens', {'tokens': bow_va_tokens}, do_compression=True)\n",
    "savemat(path_save + 'bow_va_counts', {'counts': bow_va_counts}, do_compression=True)\n",
    "del bow_va\n",
    "del bow_va_tokens\n",
    "del bow_va_counts\n",
    "\n",
    "print('Data ready !!')\n",
    "print('*************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to get the word embeddings for the data used, and save it in data/embeddings.txt\n",
    "# this is done in preProcessText2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good resources for pre-trained models:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
